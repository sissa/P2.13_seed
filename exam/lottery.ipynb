{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MHPC Deep Learning Course Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Lottery Ticket Hypothesis\n",
    "\n",
    "The aim of this exercise is to explore a somewhat misterious property of deep neural networks \n",
    "(DNN), i.e. the existence of very small subnetworks S of a given network N that are *trainable* \n",
    "and can reach the same performance of N (or even higher). \n",
    "\n",
    "This intriguing observation has been elevated to the rank of an **hypothesis** in the seminal work by Frankle & Carbin [3]\n",
    "\n",
    "https://arxiv.org/abs/1803.03635\n",
    "\n",
    "*Any large network that trains successfully contains a subnetwork that is initialized such that - when trained in isolation - it can match the accuracy of the original network in at most the same number of training iterations*\n",
    "\n",
    "The authors metaphorically called this subnetwork as a *winning ticket*.\n",
    "\n",
    "In some cases these subnetworks are really small. We can find subnetworks with the  $\\approx 1 \\%$ or less of the original connections.\n",
    "\n",
    "Imagine the possible applications of such tiny DNNs. Being small means less memory and less computation required: this can be crucial in developing embedded systems that, given a target performance to be reached, can afford it with lower resources, including energy consumption.\n",
    "\n",
    "From a theoretical viewpoint there is also another interesting angle. If these subnetworks are so small then, shall we think differently about the problem of *overparametrization*?\n",
    "\n",
    "Let us state it more clearly. A very diffuse concern about DNNs is the fact that the number of parameters is often vastly larger then the number of data points. For example we will work in this notebook with a network with $\\approx 400,000$ *trainable* parameters on the MNIST dataset, that has 60,000 training samples. This means that on average we have 7 parameters for each data point.\n",
    "If we compare this situation with polynomial regression of a dataset of 10 points, for example, a polynomial of degree 10 (that has 11 parameters) overfits the data perfectly. But in DNNs overfitting - while still there - is not as severe as we could expect.\n",
    "\n",
    "But now, if we can reduce by two orders of magnitude the number of parameters in DNNs, does this concern still hold?\n",
    "We don't know the answer, and that's *one* of the reasons why we are here.\n",
    "\n",
    "We are PyTorch beginners, but what we saw in the course is enough to approach this *research* problem in DNNs, from an empirical viewpoint. Hopefully, this will give you stimuli to explore more in depth these problems, adding your perspective to it.\n",
    "\n",
    "Another point of this exercise is to make experience of *new strategies in research and knowledge dissemination*. \n",
    "Short communications - going straight to the point - are very effective indeed as a *first* exposition to a new subject. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "In order to pass the exam you should work on some mandatory assignments.\n",
    "These are:\n",
    "\n",
    "- reading assignments\n",
    "- brief answers to the questions in the notebook\n",
    "- completion of the PyTorch code in this notebook\n",
    "\n",
    "If you want to receive full grades\n",
    "\n",
    "- do all the optional exercises in PyTorch (there are two)\n",
    "\n",
    "The optional assignments will be marked with 'OPTIONAL' in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this exercise is a mandatory *reading assignment*.\n",
    "\n",
    "You should read\n",
    "\n",
    "\n",
    "- the original paper\n",
    "https://arxiv.org/abs/1905.01067\n",
    "\n",
    "- the research account from the Uber Engineering AI team's blog:\n",
    "https://eng.uber.com/deconstructing-lottery-tickets/\n",
    "\n",
    "\n",
    "We suggest if you want to go deeper in this, to read also the original reference\n",
    "\n",
    "https://arxiv.org/abs/1803.03635\n",
    "\n",
    "After the readings, we will try to reproduce one of their numerous experiments. Take your time for this and enjoy your reading!\n",
    "\n",
    "\n",
    "![](./figs_nb/manuscript.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After the readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are confident with the ideas in the original paper\n",
    "\n",
    "https://arxiv.org/abs/1905.01067\n",
    "\n",
    "we will take it as a reference, and intertwin some excerpts from the original text and some of its figures \n",
    "with code and exercises.\n",
    "\n",
    "You may parallel now the reading of the notebook with the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "\n",
    "\n",
    "[...] A recent work by Frankle & Carbin [3] was thus surprising to many researchers when it presented\n",
    "a simple algorithm for finding sparse subnetworks within larger networks that are trainable[...] \n",
    "\n",
    "Here the key word is *trainable*: it is possible to find sparse subnetworks that perform well but they are difficult \n",
    "to train directly from scratch.\n",
    "\n",
    "\n",
    "You may want (OPTIONAL) to check the research described in the original paper\n",
    "\n",
    "https://arxiv.org/abs/1803.03635\n",
    "\n",
    "\n",
    "and a video presentation (OPTIONAL) of the original research at ICLR conference 2019\n",
    "\n",
    "https://www.youtube.com/watch?v=s7DqRZVvRiQ&t=3s\n",
    "\n",
    "\n",
    "Briefly, their approach for finding these sparse, performant networks is as follows: \n",
    "\n",
    "- train a network\n",
    "- set all weights smaller than some threshold (in absolute value) to zero\n",
    "- prune them\n",
    "- rewind the rest of the weights to their initial configuration\n",
    "- retrain the network from this starting configuration but with the zero weights frozen (not trained)\n",
    "\n",
    "\n",
    "Using this approach, they obtained two intriguing results\n",
    "\n",
    "- the pruned networks performed well\n",
    "- the network trains well only if it is rewound to its initial state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Lottery Ticket Algorithm**\n",
    "\n",
    "[...]We begin by briefly describing the lottery ticket algorithm (we simplify things a bit with respect\n",
    "to the paper):\n",
    "\n",
    "- Initialize a mask m to all ones (in the PyTorch code this will be a list of tensors of the same shapes of the ones given by model.parameters()). \n",
    "\n",
    "- Randomly initialize the parameters w of a network\n",
    "$f(x;w \\star m)$ ($\\star$ stands for elementwise multiplication), in this case of course the multiplication by the mask does not have any effect.\n",
    "\n",
    "- Train the parameters w of the network $f(x;w \\star m)$ to completion. Denote the initial weights\n",
    "before training wi and the final weights after training wf\n",
    "\n",
    "- Mask Criterion. Use the mask criterion $M(wi; wf)$ to produce a masking score for each\n",
    "currently unmasked weight. \n",
    "\n",
    "Method to create the mask: \n",
    "\n",
    "\n",
    "<span style=\"color:red\">\n",
    "Rank the weights in each layer by their scores, set the mask\n",
    "value for the top $p\\%$ to 1, the bottom $(100 - p)\\%$ to 0. \n",
    "The mask selected weights with large final value corresponding to $M(wi;wf) = |wf|$.\n",
    "</span>   \n",
    "\n",
    "\n",
    "This text is colored in red because this is what is done in the paper, but we will do it *differently*, as we will explain below!\n",
    "\n",
    "3. Mask-1 Action. Take some action with the weights with mask value 1. In [3] these weights\n",
    "were reset to their initial values and marked for training in the next round.\n",
    "\n",
    "4. Mask-0 Action. Take some action with the weights with mask value 0. In [3] these weights\n",
    "were pruned: set to 0 and frozen during any subsequent training [...]\n",
    "\n",
    "\n",
    "We do not consider iterative pruning (as in the original paper by Frankle and Carbin too): we do it just once.\n",
    "\n",
    "Other masking criteria are possible:\n",
    "\n",
    "\n",
    "![](./figs_nb/mask_criteria.png)\n",
    "\n",
    "we will stick for the moment with the original one, which is the large final (LF) mask.\n",
    "\n",
    "\n",
    "We will do experiments on a small convolutional network trained on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Here we start with the necessary imports for the PyTorch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import os\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: the Network\n",
    "\n",
    "Define the network with a class Net as we already did (check conv.py on day2 for example).\n",
    "\n",
    "To check your architecture is correct you should see something like this when printing the model:\n",
    "    \n",
    "    model = Net().to(device)\n",
    "    print(model)\n",
    "\n",
    "    Net(\n",
    "    (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
    "    (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
    "    (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
    "    (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
    "    )\n",
    "\n",
    "\n",
    "    and summary() should give something like this \n",
    "\n",
    "    summary(model, (1,28,28))\n",
    "    \n",
    "    ----------------------------------------------------------------\n",
    "        Layer (type)               Output Shape         Param #\n",
    "    ================================================================\n",
    "                Conv2d-1           [-1, 20, 24, 24]             520\n",
    "                Conv2d-2             [-1, 50, 8, 8]          25,050\n",
    "                Linear-3                  [-1, 500]         400,500\n",
    "                Linear-4                   [-1, 10]           5,010\n",
    "    ================================================================\n",
    "    Total params: 431,080\n",
    "    Trainable params: 431,080\n",
    "    Non-trainable params: 0\n",
    "    ----------------------------------------------------------------\n",
    "    Input size (MB): 0.00\n",
    "    Forward/backward pass size (MB): 0.12\n",
    "    Params size (MB): 1.64\n",
    "    Estimated Total Size (MB): 1.76\n",
    "    ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    '''\n",
    "    your code here\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check your Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "model = Net()\n",
    "model = Net().to(device)\n",
    "print(model)\n",
    "print(summary(model, (1,28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Training and test functions\n",
    "\n",
    "Before implementing the LT algorithm there are two changes we have to make to the *standard* \n",
    "training functions (as used for example in conv.py of day2)\n",
    "\n",
    "- add an optional argument *mask*\n",
    "- write the code that, if a mask is passed, modifies the update in order to freeze the parameters whose mask value is zero\n",
    "\n",
    "Notice that in PyTorch we can set the flag require_grad to *tensors*, but not to their individual elements. Let us recall this once.\n",
    "\n",
    "For tensors we could proceed as follows (we did something like this in the transfer learning, when we froze all the network but the last hidden layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the requires_grad flags\n",
    "for p in model.parameters():\n",
    "    print(p.shape, p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze first hidden layer as an example\n",
    "list(model.parameters())[0].requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the flag has changed\n",
    "for p in model.parameters():\n",
    "    print(p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us re-set to True the flag we changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.parameters())[0].requires_grad=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in order to *freeze* individual elements of our parameter tensors we have to proceed differently:\n",
    "\n",
    "- compute the gradient with respect to all the parameters \n",
    "- set to zero the gradients of the parameters whose mask value is zero\n",
    "- do the normal update\n",
    "\n",
    "\n",
    "Complete the code in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, mask=None):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        #----------------------------------------------------------------------------------------------\n",
    "        # compute the gradient with respect to all the parameters\n",
    "        '''\n",
    "        your code here\n",
    "        '''\n",
    "        \n",
    "        # set to zero the gradients of the freezed parameters if a mask is passed as a parameter        \n",
    "        '''\n",
    "        your code here\n",
    "        '''\n",
    "        \n",
    "        # parameters update\n",
    "        '''\n",
    "        your code here\n",
    "        '''\n",
    "        #----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test function is left unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader,verbose=False):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    loss /= len(test_loader.dataset)\n",
    "    acc = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            loss, correct, len(test_loader.dataset), acc))\n",
    "\n",
    "    return loss,acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to do the experiment with the LF mask.\n",
    "Before doing that we have to \n",
    "\n",
    "- set our hyperparameters\n",
    "- deal with the data\n",
    "\n",
    "Just evaluate the following two cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "#number of epochs \n",
    "epochs=5\n",
    "\n",
    "#learning rate\n",
    "lr=0.01\n",
    "\n",
    "# keep the momentum to 0, otherwise also freezed parameters \n",
    "# will move for the momentum contribution to parameters evolution\n",
    "momentum=0.0\n",
    "\n",
    "seed=1\n",
    "torch.manual_seed(seed)\n",
    "save_model=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: First Training\n",
    "\n",
    "During the first training you will\n",
    "\n",
    "- initialize the network and send to device\n",
    "- store the initial weights (wi) of the network\n",
    "- init the optimizer\n",
    "- train the network without any mask (or passing a mask of ones if you prefer)\n",
    "- store the final weights (wf)\n",
    "\n",
    "The final weights will be used later to compute the LF mask.\n",
    "\n",
    "During the training check that your loss is getting smaller and your accuracy on test set higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "if not os.path.isdir('./models'):\n",
    "    os.mkdir('./models')\n",
    "\n",
    "torch.save(model.state_dict(), 'models/initial.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    '''\n",
    "    your code here\n",
    "    '''\n",
    "    \n",
    "if (save_model):\n",
    "    print('Saving model.')\n",
    "    \n",
    "    '''\n",
    "    your code here\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masks\n",
    "\n",
    "We define different masks.\n",
    "\n",
    "- the identity mask, which is useful for debugging: it will left the training unaltered\n",
    "- the random mask, that we will use to make comparisons\n",
    "- the LF (large final) mask\n",
    "\n",
    "The first two are given, the implementation of the LF mask is an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_mask(model):\n",
    "    '''\n",
    "    Returns the identity mask for all parameter tensors in a list\n",
    "    '''\n",
    "    mask = []\n",
    "    for p in model.parameters():\n",
    "        mask.append(torch.ones_like(p))\n",
    "    return mask\n",
    "\n",
    "def random_mask(model, level=0.0):\n",
    "    '''\n",
    "    Construct random mask with a given level of pruning (probability to assign a zero value)\n",
    "    '''\n",
    "    # construct random mask\n",
    "    mask = []\n",
    "    frac = 0\n",
    "    tot = 0\n",
    "    for i,p in enumerate(model.parameters()):\n",
    "        mask.append( (torch.rand_like(p) > level).float().to(device) )\n",
    "        frac += torch.sum( mask[i] ).item() \n",
    "        tot += mask[i].numel()\n",
    "    frac = frac/tot\n",
    "    return mask, frac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: the LF Mask\n",
    "\n",
    "The original LF mask (as described in the introduction in the red text) is left as an **OPTIONAL Exercise 1 (see at the end of this notebook)**.\n",
    "You will implement here a *variant* of the original LF (large final) mask. \n",
    "\n",
    "The general idea is just to mark with 1 the weights that at the end of the training are \n",
    "larger then a certain threshold. This is explained in the follwing figure\n",
    "\n",
    "![](./figs_nb/LF_mask.png)\n",
    "\n",
    "Concretely you will do it as follows. Let us denote with\n",
    "\n",
    "- p a vector that contains all the weights\n",
    "- |p| its elementwise absolute value\n",
    "- m the average of |p|\n",
    "- s the standard deviation of p (notice that now we are considering the original parameters, not their absolute values)\n",
    "- $\\alpha$ is a parameter in the range $[0,2]$\n",
    "\n",
    "Then a parameter $p_i$ will be considered relevant (mask = 1 for this parameter) if its final value is such that:\n",
    "\n",
    "$$\n",
    "|p_i| > m + \\alpha \\times s\n",
    "$$\n",
    "\n",
    "Your function (LF_mask) will have the following signature:\n",
    "    \n",
    "    input : model,alpha\n",
    "        \n",
    "    output : mask (a list of tensors with the same shaper of list(model.parameters()) ) in which to \n",
    "             each parameter is associate a value 1 if relevant, 0 if irrelevant\n",
    "             \n",
    "             frac (a the fraction of parameters whose mask is equal to 1)\n",
    "             \n",
    "             The latter is useful to keep track of the level of parameters'pruning which is \n",
    "             implicitly set by alpha\n",
    "             \n",
    "Then we will test it and see how many parameters we pruned with a certain $\\alpha$.\n",
    "             \n",
    "Hint: Basically you will have to concatenate all the parameters in a long numpy array first, in order to compute the statistics you need. Maybe you will find useful the functions torch.where, torch.ones_like, torch.zeros_like. Furthermore, before acting on torch tensors you will need to detach them from the computational graph (see detach() method), send them to the cpu device, and convert them into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LF_mask(model,alpha):\n",
    "    '''\n",
    "    Construct large final (LF) mask. The threshold for \n",
    "    decision is determined globally.\n",
    "    '''\n",
    "    frac = 0\n",
    "    tot = 0\n",
    "    \n",
    "    \n",
    "    # concatenate all parameters into a numpy array\n",
    "    '''\n",
    "    your code here\n",
    "    '''\n",
    "       \n",
    "    # compute mean of the absolute values and standard deviation         \n",
    "    '''\n",
    "    your code here\n",
    "    '''\n",
    "        \n",
    "        \n",
    "    # compute the mask\n",
    "    mask = []\n",
    "    for i, p in enumerate( model.parameters() ):\n",
    "        \n",
    "        '''\n",
    "        your code here\n",
    "        '''\n",
    "        \n",
    "        mask.append(...)\n",
    "        \n",
    "        frac += torch.sum( mask[i] ).item()\n",
    "        tot += mask[i].numel()\n",
    "\n",
    "    frac = frac/tot\n",
    "    \n",
    "    return mask, frac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test the LF mask\n",
    "\n",
    "To make a test let us create the mask with the LF function, then print the fraction of weights that received \n",
    "a mask equal to 1.\n",
    "Notice that with $\\alpha=1$ you are already pruning a substantial part of your network ($\\approx 98 \\%$ of the parameters).\n",
    "\n",
    "This means that the LF mask defines an highly *sparse* subnetwork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('models/final.pt') )\n",
    "alpha = 1\n",
    "with torch.no_grad():\n",
    "    mask, frac = LF_mask(model,alpha)\n",
    "print('Fraction of weights with mask=1:  {}'.format(np.round(frac,3)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Retraining with the LF mask\n",
    "\n",
    "Now we will verify that this highly sparse subnetwork is also *trainable* when if start from the original weights.\n",
    "    \n",
    "We will\n",
    "\n",
    "- rewind the network to its initial state\n",
    "- apply the FL mask *before* training: in this way we will obtain our sparse subnetwork\n",
    "- retrain just the subnetwork (it is enough to pass the mask to the train function: convince yourself that this is indeed the case)\n",
    "- evaluate it at the end\n",
    "\n",
    "Verify that the subnetwork is trainable if we start from the original weights (how much accuracy is \n",
    "reached on the test set?).\n",
    "What happens if we start from a new initialization? Verify also that the subnetwork is\n",
    "not as good as before in this case (how much accuracy do you reach now? Notice that in this context if a subnetwork reaches *only* the $\\approx 90 \\%$ - that one might think is not a bad result after all - of performance we consider it as *not trainable*, meaning only that the training is not completely effective).\n",
    "\n",
    "We do the initial step and you will complete the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewind\n",
    "'''\n",
    "your code here\n",
    "'''\n",
    "\n",
    "# apply mask\n",
    "'''\n",
    "your code here\n",
    "'''\n",
    "    \n",
    "# re-instantiate optimizer\n",
    "'''\n",
    "your code here\n",
    "'''\n",
    "\n",
    "# train\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    '''\n",
    "    your code here\n",
    "    '''\n",
    "    \n",
    "# evaluate at the end of training\n",
    "_, acc_trained_subnetwork = test(model, device, test_loader)\n",
    "\n",
    "print('Accuracy of the subnetwork after the training: {}'.format(acc_trained_subnetwork) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewind to a random state\n",
    "model = Net().to(device)\n",
    "\n",
    "# apply mask\n",
    "'''\n",
    "your code here\n",
    "'''\n",
    "    \n",
    "# re-instantiate optimizer\n",
    "'''\n",
    "your code here\n",
    "'''\n",
    "\n",
    "# train\n",
    "for epoch in tqdm(range(1, epochs + 1)):\n",
    "    '''\n",
    "    your code here\n",
    "    '''\n",
    "    \n",
    "# evaluate at the end of training\n",
    "_, acc_trained_subnetwork = test(model, device, test_loader)\n",
    "\n",
    "print('Accuracy of the subnetwork after the training: {}'.format(acc_trained_subnetwork) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: The LF is actually a very good mask (not still the SuperMask but...)\n",
    "\n",
    "Let us read again this sentence from the paper in section **5 Supermasks**:\n",
    "\n",
    "[...] The hypothesis above suggests that for certain mask criteria, like large final, that masking is training:\n",
    "the masking operation tends to move weights in the direction they would have moved during training.\n",
    "\n",
    "If so, just how powerful is this training operation? To answer this question, we can start all the way\n",
    "from the beginningâ€”not training the network at all, but simply applying a mask to the randomly\n",
    "initialized network.\n",
    "\n",
    "It turns out that with a well-chosen mask, an untrained network can already attain a test accuracy\n",
    "far better than chance. This might come as a surprise, because if you use a randomly initialized and\n",
    "untrained network to, say, classify images of handwritten digits from the MNIST dataset, you would\n",
    "expect accuracy to be no better than chance (about $10 \\%$). But now imagine you multiply the network\n",
    "weights by a mask containing only zeros and ones. In this instance, weights are either unchanged or\n",
    "deleted entirely, but the resulting network now achieves nearly 40 percent accuracy at the task! This\n",
    "is strange, but it is exactly what we observe with masks created using the large final criterion [...]\n",
    "\n",
    "\n",
    "First of all two questions:\n",
    "\n",
    "- what is the hypothesis the authors are referring to at the beginning of this excerpt?\n",
    "- why is, in your opinion, reasonable that you could - at least in principle - obtain better than chance results applying the LF mask to a randomly initialized network?\n",
    "\n",
    "Write short answers to this questions below.\n",
    "\n",
    "### Answer 1\n",
    "\n",
    "### Answer 2\n",
    "\n",
    "Our aim is now to reproduce the phenomenon evidenced in this plot\n",
    "\n",
    "![](./figs_nb/supermask.png)\n",
    "\n",
    "We do not want to reproduce the numbers but just the *masking is training* effect.\n",
    "\n",
    "We will perform these steps:\n",
    "\n",
    "- rewind our network to its initial state\n",
    "- evaluate it once\n",
    "- apply the FL mask\n",
    "- repeat the evaluation on the subnetwork\n",
    "\n",
    "Consider that we are not re-training our network but just applying the LF mask.\n",
    "\n",
    "Then do the same for a random mask, creating one, with a level of pruning that\n",
    "matches the one found for the LF mask.\n",
    "\n",
    "    Hint: you should use \n",
    "    \n",
    "    rmask, rfrac = random_mask(model, 1-frac)\n",
    "    \n",
    "    where frac is computed on the LF mask\n",
    "\n",
    "What do you see? Are these results qualitatively similar to the ones plotted above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewind\n",
    "'''\n",
    "your code here\n",
    "'''\n",
    "\n",
    "# eval\n",
    "_, acc_init = test(model, device, test_loader)\n",
    "\n",
    "print('Accuracy of the randomly initialized network: {}'.format(acc_init) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply mask\n",
    "'''\n",
    "your code here\n",
    "'''\n",
    "    \n",
    "# eval the sub-network\n",
    "_, acc_subnetwork = test(model, device, test_loader)\n",
    "\n",
    "print('Accuracy of the randomly initialized subnetwork: {}'.format(acc_subnetwork) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random mask\n",
    "'''\n",
    "your code here\n",
    "'''\n",
    "\n",
    "print('Fraction of weights with mask=1:  {}'.format(np.round(rfrac,3)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewind\n",
    "'''\n",
    "your code here\n",
    "'''\n",
    "\n",
    "# apply random mask\n",
    "'''\n",
    "your code here\n",
    "'''\n",
    "\n",
    "# eval the random sub-network\n",
    "_, acc_rand_subnetwork = test(model, device, test_loader)\n",
    "\n",
    "print('Accuracy of the randomly initialized network, with a random mask: {}'.format(acc_rand_subnetwork) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us stop for a moment and think at what we obtained so far.\n",
    "With this version of LF masking we were able to identify a sub-network S \n",
    "of the original network N (orders of magnitude smaller) that had the following properties\n",
    "\n",
    "- it was *trainable*, recovering almost the original performance\n",
    "- was still very good in recognizing digits (well above chance) *without* any training\n",
    "\n",
    "These are amazing result, isn't it?\n",
    "\n",
    "What is left, for our experiments, is to see how far can we go with the pruning. At the moment we\n",
    "had a subnetwork whose size was about $\\approx 2\\%$ of the original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: How far can we go with the miniaturization?\n",
    "\n",
    "Then next task is to understand how much we can shrink our network without\n",
    "substantial harm.\n",
    "\n",
    "We will do the same analysis as in Exercise 5 in a sequence of increasing $\\alpha$ from 0 to 2, and \n",
    "than we will plot results.\n",
    "\n",
    "The useful plots are fraction of surviving weighs vs test accuracy, like the ones in this plot\n",
    "\n",
    "![](./figs_nb/plot_example.png)\n",
    "\n",
    "\n",
    "(but notice that your plots will not start from $100 \\%$ weights remaining, because for $\\alpha=0$ you are\n",
    "already pruning weights...)\n",
    "\n",
    "\n",
    "For each value of alpha do just one run (we will not compute error bars because it would take too long).\n",
    "At the end you will have a plot with just one curve, without error bars.\n",
    "\n",
    "If it takes to long just check a few values of $\\alpha$, \n",
    "for example $\\alpha=0.5,1.0,1.5$ and print a list of (fraction, performance)\n",
    "\n",
    "You will have to complete the function\n",
    "\n",
    "    alpha_training\n",
    "   \n",
    "below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_levels=10\n",
    "alphas = np.linspace(0,4,num_levels)\n",
    "\n",
    "# uncomment this if it takes too long\n",
    "#alphas = np.array([0.5, 1.0, 1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_training(model, alpha, epochs):\n",
    "    \n",
    "    # load final weights and compute mask at level alpha\n",
    "    '''\n",
    "    your code here\n",
    "    '''\n",
    "        \n",
    "    # rewind\n",
    "    '''\n",
    "    your code here\n",
    "    '''\n",
    "        \n",
    "    # apply mask\n",
    "    '''\n",
    "    your code here\n",
    "    '''\n",
    "    \n",
    "    # reinit optimizer\n",
    "    '''\n",
    "    your code here\n",
    "    '''\n",
    "    \n",
    "    # train\n",
    "    stats = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        '''\n",
    "        your code here\n",
    "        '''\n",
    "    stats = np.array(stats)\n",
    "        \n",
    "    return stats,frac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = []\n",
    "Fracs = []\n",
    "for alpha in tqdm(alphas):  \n",
    "    stats, fracs = alpha_training(model, alpha, epochs)\n",
    "    Stats.append(stats)\n",
    "    Fracs.append(fracs)\n",
    "Stats = np.asarray(Stats)\n",
    "Fracs = np.asarray(Fracs)\n",
    "np.save('models/Stats_LF_mask.npy', Stats)\n",
    "np.save('models/Fracs_LF_mask.npy', Fracs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "plt.semilogx(Fracs*100, Stats[:,-1,-1],'-bo')\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('weights remaining %')\n",
    "plt.ylim([90,100])\n",
    "plt.gca().invert_xaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8: Compare LF mask with random masking\n",
    "\n",
    "Then next task is to compare the efficiency of LF mask with respect to random masking. This goes as before, the only difference being how you compute the mask.\n",
    "You will just have to complete the function\n",
    "    \n",
    "    random_training\n",
    "    \n",
    "    \n",
    "When you finish plot the results of the LF mask and the random mask together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training(model, level, epochs):\n",
    "    \n",
    "    # compute mask at a pruning level\n",
    "    '''\n",
    "    your code here\n",
    "    '''\n",
    "        \n",
    "    # rewind\n",
    "    '''\n",
    "    your code here\n",
    "    '''\n",
    "        \n",
    "    # apply mask\n",
    "    '''\n",
    "    your code here\n",
    "    '''\n",
    "    \n",
    "    # reinit optimizer\n",
    "    '''\n",
    "    your code here\n",
    "    '''\n",
    "    \n",
    "    # train\n",
    "    stats = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        '''\n",
    "        your code here\n",
    "        '''\n",
    "    stats = np.array(stats)\n",
    "        \n",
    "    return stats,frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the levels of pruning here, from 0 to 1\n",
    "pruning_levels = np.linspace(0,1,num_levels)\n",
    "print(pruning_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = []\n",
    "Fracs = []\n",
    "\n",
    "for level in tqdm(pruning_levels):  \n",
    "    stats, fracs = random_training(model, level, epochs)\n",
    "    Stats.append(stats)\n",
    "    Fracs.append(fracs)\n",
    "Stats = np.asarray(Stats)\n",
    "Fracs = np.asarray(Fracs)\n",
    "\n",
    "np.save('models/Stats_random_mask.npy', Stats)\n",
    "np.save('models/Fracs_random_mask.npy', Fracs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stats = np.load('models/Stats_LF_mask.npy')\n",
    "Fracs = np.load('models/Fracs_LF_mask.npy')\n",
    "Stats_rand = np.load('models/Stats_random_mask.npy')\n",
    "Fracs_rand = np.load('models/Fracs_random_mask.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "plt.semilogx(Fracs*100, Stats[:,-1,-1],'-bo',label='LF mask (freeze at zero)')\n",
    "# we leave out the last point if the frac is 0, in order to avoid the log(0)\n",
    "plt.semilogx(Fracs_rand[:-1]*100, Stats_rand[:-1,-1,-1],'-ro',label='random mask')\n",
    "plt.ylabel('test accuracy')\n",
    "plt.xlabel('weights remaining %')\n",
    "plt.xlim([0, 110])\n",
    "plt.ylim([90,100])\n",
    "plt.gca().invert_xaxis()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be clear from this plot that \n",
    "\n",
    "- if we do random masking, pruning more than $20 \\%$ of the parameters severely affect the network performance \n",
    "- if we use the mask given by the LF criterion, we can prune much more without affecting the performance sensibly\n",
    "\n",
    "This means that, for example, we could endow an embedded system - at least in principle - with a tenfold smaller network just by using a very simple masking method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "We hope that this exercise was a stimulus for you to delve more into this subject, and more \n",
    "generally into deep learning problems.\n",
    "\n",
    "Good Luck!!!\n",
    "\n",
    "![](./figs_nb/Thats_all_folks.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL ASSIGNMENTS\n",
    "\n",
    "If you want to receive full grades you should work on the following two exercises.\n",
    "\n",
    "<span style=\"color:blue\">These exercises are just sketched: incomplete but reasonable attempts to solve them will be considered for evaluation. Variations on these themes, inspired by your curiosity (for example in the case of Exercise 1 you could also implement another type of mask, I would suggest in this case the *magnitude increase* mask), will be considered equivalently.</span>\n",
    "\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "Implement the original version of the LF mask (or another one of your choice).\n",
    "Check that it works for a few values of the pruning level and if you have time/will plot it as a function of pruning level.\n",
    "\n",
    "### Exercise 2\n",
    "\n",
    "Pruning means setting exactly to zero the weigths we want to eliminate.\n",
    "What if, on the contrary, we keep these weigths frozen at their initialization values? \n",
    "\n",
    "In this case notice that we are *not* pruning the network.\n",
    "In order to do that there is just a very simple thing to do (or not do) in the function alpha_training.\n",
    "Find it, make this new experiment and plot all the \n",
    "three results together (with LF masking, random masking and this version). \n",
    "\n",
    "(You can do that with both versions of the LF masking: the original version of the optional Exercise 1 or our version: choose whatever you want).\n",
    "\n",
    "If you want you could also add a test to see that the freezed parameters \n",
    "do not move from their initial values, by constructing a suitable alert function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
